{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c942955-d73d-4508-8e26-6e67a5734e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0badb-44e2-476f-9847-a21c24b4987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import smart_resize\n",
    "\n",
    "database = pd.read_excel('/Final Table- Process V').drop([0,5,10,13])\n",
    "source_index = database['Source Index'].dropna().tolist()\n",
    "dataset_index = database['Dataset Index'].dropna().tolist()\n",
    "random_gain_dict = {}\n",
    "diverse_gain_dict = {}\n",
    "\n",
    "for i in range(len(source_index)):\n",
    "\n",
    "  group_name = source_index[i]\n",
    "  datasets = [dat.strip() for dat in dataset_index[i].split(',')]\n",
    "  for dat in datasets:\n",
    "    subgroup_name = dat\n",
    "\n",
    "    try:\n",
    "      with h5py.File(destination_hdf5_file, 'r') as hdf:\n",
    "        subgroup = hdf[group_name][subgroup_name]\n",
    "        if 'processed' in subgroup:\n",
    "          dataset = subgroup['processed']\n",
    "        else:\n",
    "          dataset = subgroup['rawdata']\n",
    "        data_array = np.array(dataset)\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "                print(f\"Running on GPU: {gpus[0].name}\")\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"No GPU found. Running on CPU.\")\n",
    "\n",
    "        def extract_features(img_array):\n",
    "            img_array = np.stack((img_array,) * 3, axis=-1)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            img_array = smart_resize(img_array, (224,224))\n",
    "            features = model.predict(img_array, verbose=0)\n",
    "            return features.flatten()\n",
    "\n",
    "        model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "        image_features = []\n",
    "        num_images = data_array.shape[0]\n",
    "        for i in range(num_images):\n",
    "            img_array = data_array[i]\n",
    "            features = extract_features(img_array)\n",
    "            image_features.append(features)\n",
    "        image_features = np.array(image_features)\n",
    "\n",
    "      kmeans = KMeans(n_clusters=20, n_init = 10, random_state=42).fit(image_features)\n",
    "      labels = kmeans.labels_\n",
    "\n",
    "      # Select equal number of images from each cluster to get a subset of 1000 images\n",
    "      cluster_counts = np.bincount(labels)\n",
    "      min_images_per_cluster = 1000 // 20\n",
    "\n",
    "      selected_images = []\n",
    "      for cluster_idx in range(20):\n",
    "          cluster_images = np.where(labels == cluster_idx)[0]\n",
    "          selected_indices = np.random.choice(cluster_images, min(min_images_per_cluster,len(cluster_images)), replace=False)\n",
    "          selected_images.extend(selected_indices)\n",
    "\n",
    "      # Randomly select images if there are remaining slots\n",
    "      # remaining_slots = 1000 - len(selected_images)\n",
    "      # if remaining_slots > 0:\n",
    "          # remaining_indices = np.setdiff1d(range(len(labels)), selected_images)\n",
    "          # additional_indices = np.random.choice(remaining_indices, remaining_slots, replace=False)\n",
    "          # selected_images.extend(additional_indices)\n",
    "\n",
    "      selected_image_indices = selected_images\n",
    "\n",
    "      print(group_name,subgroup_name)\n",
    "\n",
    "\n",
    "      # Measure diversity using distance gains\n",
    "      selected_features = image_features[selected_image_indices]\n",
    "      distances = pdist(selected_features, 'euclidean')\n",
    "      distance_matrix = squareform(distances)\n",
    "\n",
    "      # Calculate the average distance gain\n",
    "      average_distance_gain = np.mean(distances)\n",
    "\n",
    "      print(f\"Average optimized Distance Gain: {average_distance_gain}\")\n",
    "      key = f\"{group_name}_{subgroup_name}\"\n",
    "      # Append average_distance_gain to the dictionary\n",
    "      diverse_gain_dict[key] = average_distance_gain\n",
    "\n",
    "      random_sample_indices = np.random.choice(image_features.shape[0], selected_features.shape[0], replace=False)  # Random sample indices\n",
    "      random_sample_features = image_features[random_sample_indices]\n",
    "      distances = pdist(random_sample_features, 'euclidean')\n",
    "      distance_matrix = squareform(distances)\n",
    "      # Calculate the average distance gain\n",
    "      average_distance_gain = np.mean(distances)\n",
    "\n",
    "      print(f\"Average random Distance Gain: {average_distance_gain}\")\n",
    "      random_gain_dict[key] = average_distance_gain\n",
    "\n",
    "      file_name = f\"{group_name}_{subgroup_name}.npy\"\n",
    "      # Define the path to save the .npy file\n",
    "      save_path = f'/content/drive/MyDrive/MeltPoolViT/Diverse_index_KDPP/{file_name}'\n",
    "\n",
    "# Define the filename for the JSON file\n",
    "json_filename = \"/Diverse_index_KDPP2/random.json\"\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(random_gain_dict, json_file)\n",
    "\n",
    "print(f\"Dictionary saved to {json_filename}\")\n",
    "\n",
    "# Define the filename for the JSON file\n",
    "json_filename = \"/Diverse_index_KDPP2/diverse.json\"\n",
    "\n",
    "# Save the dictionary as a JSON file\n",
    "with open(json_filename, 'w') as json_file:\n",
    "    json.dump(diverse_gain_dict, json_file)\n",
    "\n",
    "print(f\"Dictionary saved to {json_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23662197-5202-4992-afb9-c4255f2d1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Display three images from each cluster\n",
    "def display_three_images_per_cluster(image_array, labels, num_clusters, images_per_cluster=3):\n",
    "    # Determine the number of rows and columns for subplots\n",
    "    num_rows = num_clusters\n",
    "    num_cols = images_per_cluster\n",
    "\n",
    "    # Adjust the figure size for larger images\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols * 4, num_rows * 4))  # Adjust size as needed\n",
    "\n",
    "    if num_clusters == 1:\n",
    "        axes = np.array([axes])  # Ensure axes is iterable even for a single cluster\n",
    "    else:\n",
    "        axes = np.array(axes).reshape((num_rows, num_cols))  # Reshape axes for multiple clusters\n",
    "\n",
    "    for cluster_idx in range(num_clusters):\n",
    "        # Find the indices of images in the current cluster\n",
    "        cluster_indices = np.where(labels == cluster_idx)[0]\n",
    "\n",
    "        # Select three images from the cluster\n",
    "        selected_indices = np.random.choice(cluster_indices, min(images_per_cluster, len(cluster_indices)), replace=False)\n",
    "\n",
    "        for img_idx, sample_index in enumerate(selected_indices):\n",
    "            img_array = image_array[sample_index]\n",
    "            img_pil = img_array\n",
    "\n",
    "            ax = axes[cluster_idx, img_idx]\n",
    "            ax.imshow(img_pil)\n",
    "            ax.set_title(f\"Cluster {cluster_idx}\")\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display three images from each cluster\n",
    "display_three_images_per_cluster(data_array, labels, optimal_clusters, images_per_cluster=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9e41c-0048-4e8e-a752-0421510609f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(source_index)):\n",
    "\n",
    "  group_name = source_index[i]\n",
    "  datasets = [dat.strip() for dat in dataset_index[i].split(',')]\n",
    "  for dat in datasets:\n",
    "    subgroup_name = dat\n",
    "\n",
    "    try:\n",
    "      with h5py.File(destination_hdf5_file, 'r') as hdf:\n",
    "        subgroup = hdf[group_name][subgroup_name]\n",
    "        if 'processed' in subgroup:\n",
    "          dataset = subgroup['processed']\n",
    "        else:\n",
    "          dataset = subgroup['rawdata']\n",
    "        data_array = np.array(dataset)\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "                print(f\"Running on GPU: {gpus[0].name}\")\n",
    "            except RuntimeError as e:\n",
    "                print(e)\n",
    "        else:\n",
    "            print(\"No GPU found. Running on CPU.\")\n",
    "\n",
    "        def extract_features(img_array):\n",
    "            img_array = np.stack((img_array,) * 3, axis=-1)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            img_array = smart_resize(img_array, (224,224))\n",
    "            features = model.predict(img_array, verbose=0)\n",
    "            return features.flatten()\n",
    "\n",
    "        model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "        image_features = []\n",
    "        num_images = data_array.shape[0]\n",
    "        for i in range(num_images):\n",
    "            img_array = data_array[i]\n",
    "            features = extract_features(img_array)\n",
    "            image_features.append(features)\n",
    "        image_features = np.array(image_features)\n",
    "\n",
    "      DPP = FiniteDPP('likelihood', **{'L': image_features.dot(image_features.T)})\n",
    "\n",
    "      k = 1000\n",
    "      for _ in range(1):\n",
    "          DPP.sample_mcmc_k_dpp(size=k, random_state=42)\n",
    "\n",
    "      print(DPP.list_of_samples)\n",
    "\n",
    "      selected_image_indices = np.array(DPP.list_of_samples[0][1])\n",
    "\n",
    "      print(group_name,subgroup_name)\n",
    "\n",
    "\n",
    "      # Measure diversity using distance gains\n",
    "      selected_features = image_features[selected_image_indices]\n",
    "      distances = pdist(selected_features, 'euclidean')\n",
    "      distance_matrix = squareform(distances)\n",
    "\n",
    "      # Calculate the average distance gain\n",
    "      average_distance_gain = np.mean(distances)\n",
    "\n",
    "      print(f\"Average optimized Distance Gain: {average_distance_gain}\")\n",
    "      key = f\"{group_name}_{subgroup_name}\"\n",
    "      # Append average_distance_gain to the dictionary\n",
    "      diverse_gain_dict[key] = average_distance_gain\n",
    "\n",
    "      random_sample_indices = np.random.choice(image_features.shape[0], selected_features.shape[0], replace=False)  # Random sample indices\n",
    "      random_sample_features = image_features[random_sample_indices]\n",
    "      distances = pdist(random_sample_features, 'euclidean')\n",
    "      distance_matrix = squareform(distances)\n",
    "      # Calculate the average distance gain\n",
    "      average_distance_gain = np.mean(distances)\n",
    "\n",
    "      print(f\"Average random Distance Gain: {average_distance_gain}\")\n",
    "      random_gain_dict[key] = average_distance_gain\n",
    "\n",
    "      file_name = f\"{group_name}_{subgroup_name}.npy\"\n",
    "      # Define the path to save the .npy file\n",
    "      save_path = f'/content/drive/MyDrive/MeltPoolViT/Diverse_index2/{file_name}'\n",
    "\n",
    "      np.save(save_path, selected_image_indices)\n",
    "    except:\n",
    "      print(f\"Error at {group_name}, {subgroup_name}\")\n",
    "      continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cf31c5-6ce1-482c-8d36-9e09e2640592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "pathes = ['/Diverse_index','/Diverse_index_KDPP','/Diverse_index_KDPP2']\n",
    "results = {}\n",
    "for path in pathes:\n",
    "  i = 0\n",
    "  all = 0\n",
    "  with open(path+'/random.json', 'r') as file:\n",
    "    random = json.load(file)\n",
    "  with open(path+'/diverse.json', 'r') as file:\n",
    "    diverse = json.load(file)\n",
    "\n",
    "    for key in random.keys():\n",
    "      i = i+ 1\n",
    "      all = all + diverse[key]/random[key]\n",
    "      if key not in results.keys():\n",
    "        results[key] = [diverse[key]/random[key]]\n",
    "      else:\n",
    "        results[key].append(diverse[key]/random[key])\n",
    "      print(key, ' :', diverse[key]/random[key])\n",
    "\n",
    "    print('mean: ', all/i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e25340-d247-4aae-b972-fb5658b1cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from PIL import Image\n",
    "\n",
    "def cosine_similarity_images(image_path1, image_path2):\n",
    "    # Load images\n",
    "    img1 = Image.open(image_path1)\n",
    "    img2 = Image.open(image_path2)\n",
    "\n",
    "    # Convert images to numpy arrays\n",
    "    img_array1 = np.array(img1).reshape(1, -1)\n",
    "    img_array2 = np.array(img2).reshape(1, -1)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(img_array1, img_array2)\n",
    "\n",
    "    return similarity[0][0]\n",
    "\n",
    "from vendi_score import vendi\n",
    "\n",
    "vendi.score(selected, cosine_similarity_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
